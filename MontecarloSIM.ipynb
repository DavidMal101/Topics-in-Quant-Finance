{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.special import ndtri\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy import interpolate\n",
    "from scipy.interpolate import LinearNDInterpolator as LNDI\n",
    "import itertools\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "%precision 16"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the necessary data and structure it as desired:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#back out moneyness from delta\n",
    "def inverse_delta(delta,tao,r,sigma):\n",
    "    '''\n",
    "    Calculate the moneyness of the option given: \n",
    "    - delta the BS delta, \n",
    "    - tao   time to expiry, \n",
    "    - sigma implied vol, \n",
    "    - r     interest rate.\n",
    "    '''\n",
    "    d1 = ndtri(delta)\n",
    "    expr = d1 * sigma * np.sqrt(tao) - tao * (r + sigma**2 / 2)\n",
    "    m = np.exp(-expr)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using forward slashes\n",
    "volsurf_df = pd.read_csv(\"C:/Users/Usuario/Documents/MSc_Maths_Finance/Topics_in_Quantitative_Finance/coursework/Topics-in-Quant-Finance/implied vol with delta/implied vol with delta.csv\")\n",
    "zero_curve = pd.read_csv(\"C:/Users/Usuario/Documents/MSc_Maths_Finance/Topics_in_Quantitative_Finance/coursework/Topics-in-Quant-Finance/implied vol with delta/Zero Coupon Curve.csv\")\n",
    "\n",
    "#options with less than 10 days to expiry have no implied volatility calculated\n",
    "volsurf_df = volsurf_df[(volsurf_df['days'] != 10) & (volsurf_df['days'] <= 365)] #shouldnt then this be <= 10 or use dropna\n",
    "volsurf_df = volsurf_df.drop(columns=['index_flag','secid'])\n",
    "\n",
    "#scale delta to be between 0 and 1\n",
    "volsurf_df['delta'] = volsurf_df['delta']/100\n",
    "\n",
    "#interest measured in percentage or basis points?\n",
    "zero_curve['rate'] = zero_curve['rate']/100\n",
    "\n",
    "volsurf_df = volsurf_df.drop('cp_flag', axis=1)\n",
    "volsurf_df = volsurf_df.drop('ticker', axis=1)\n",
    "\n",
    "impl_vol_sample = volsurf_df[volsurf_df['date']=='2021-12-31']\n",
    "\n",
    "zero_curve_sample = zero_curve[zero_curve['date'] == '2021-12-31']\n",
    "zero_curve_sample = zero_curve_sample[zero_curve_sample['days'].isin([30,60,91,122,152,182,273,365])]\n",
    "\n",
    "impl_vol_sample = pd.merge(impl_vol_sample,zero_curve_sample[['days','rate']], how='left', left_on='days', right_on='days')\n",
    "\n",
    "impl_vol_sample['moneyness']=impl_vol_sample.apply(lambda x: inverse_delta(x.delta,x.days/365,x.rate,x.impl_volatility),axis=1)\n",
    "\n",
    "impl_vol_sample = impl_vol_sample.drop('rate', axis=1)\n",
    "impl_vol_sample = impl_vol_sample.drop('delta', axis=1)\n",
    "impl_vol_sample = impl_vol_sample.drop('date', axis=1)\n",
    "impl_vol_sample['tau'] = impl_vol_sample['days']/365\n",
    "impl_vol_sample = impl_vol_sample.drop('days', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>impl_volatility</th>\n",
       "      <th>moneyness</th>\n",
       "      <th>tau</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.106347</td>\n",
       "      <td>1.040784</td>\n",
       "      <td>0.082192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.106598</td>\n",
       "      <td>1.033114</td>\n",
       "      <td>0.082192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.108139</td>\n",
       "      <td>1.027378</td>\n",
       "      <td>0.082192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.110721</td>\n",
       "      <td>1.022602</td>\n",
       "      <td>0.082192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.114071</td>\n",
       "      <td>1.018286</td>\n",
       "      <td>0.082192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0.245869</td>\n",
       "      <td>0.912286</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>0.263068</td>\n",
       "      <td>0.872906</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>0.283304</td>\n",
       "      <td>0.825804</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>0.307582</td>\n",
       "      <td>0.767530</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>0.336577</td>\n",
       "      <td>0.692261</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>136 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     impl_volatility  moneyness       tau\n",
       "0           0.106347   1.040784  0.082192\n",
       "1           0.106598   1.033114  0.082192\n",
       "2           0.108139   1.027378  0.082192\n",
       "3           0.110721   1.022602  0.082192\n",
       "4           0.114071   1.018286  0.082192\n",
       "..               ...        ...       ...\n",
       "131         0.245869   0.912286  1.000000\n",
       "132         0.263068   0.872906  1.000000\n",
       "133         0.283304   0.825804  1.000000\n",
       "134         0.307582   0.767530  1.000000\n",
       "135         0.336577   0.692261  1.000000\n",
       "\n",
       "[136 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "impl_vol_sample"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell we choose the parameters or our basis functions which are:\n",
    "1) f1 is the level, constant value,\n",
    "2) f2 is the skew of the surface, which depends only on moneyness linearly,\n",
    "3) f3 is the curvature of the surface and is a quadratic polynomial in moneyness and linear in tau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of vol values or f1: 0.185976\n",
      "Optimal fit for f2: -0.538764 * m + 0.721013\n",
      "Optimal fit for f3: (1.123267 * m**2 + -3.514570 * m + 2.863176) * (0.076365 * tau + 0.346212)\n"
     ]
    }
   ],
   "source": [
    "data = impl_vol_sample.to_numpy()\n",
    "\n",
    "# Separate the data into separate arrays for m, tau, and vol\n",
    "m_values = data[:, 1]\n",
    "tau_values = data[:, 2]\n",
    "vol_values = data[:, 0]\n",
    "\n",
    "# Define the functions you want to fit to your data\n",
    "def f2(x, a, b):\n",
    "    return a*x + b\n",
    "\n",
    "def f3(xy, a, b, c, d, e):\n",
    "    x, y = xy\n",
    "    return (a*x**2 + b*x + c) * (d*y + e)\n",
    "\n",
    "# Find the optimal fit for f1 using curve_fit\n",
    "popt2, pcov2 = curve_fit(f2, m_values, vol_values)\n",
    "\n",
    "# Find the optimal fit for f2 using curve_fit\n",
    "popt3, pcov3 = curve_fit(f3, (m_values, tau_values), vol_values)\n",
    "\n",
    "# Calculate the mean of the vol values\n",
    "f1_mean = np.mean(vol_values)\n",
    "\n",
    "# Print the results\n",
    "print(\"Mean of vol values or f1: {:.6f}\".format(f1_mean))\n",
    "print(\"Optimal fit for f2: {:.6f} * m + {:.6f}\".format(*popt2))\n",
    "print(\"Optimal fit for f3: ({:.6f} * m**2 + {:.6f} * m + {:.6f}) * ({:.6f} * tau + {:.6f})\".format(*popt3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call(m, tau, sigma, r):\n",
    "    '''\n",
    "    Function that retuns the price of a European Call given:\n",
    "    - m     moneyness\n",
    "    - tau   time to maturity\n",
    "    - sigma volatility\n",
    "    - r     interest rate\n",
    "    '''\n",
    "    d1 = (np.log(m)+ tau * (r + sigma**2 / 2)) / (sigma * np.sqrt(tau))\n",
    "    d2 = (np.log(m)+ tau * (r - sigma**2 / 2)) / (sigma * np.sqrt(tau))\n",
    "    return norm.cdf(d1) - m * np.exp(-r * tau) * norm.cdf(d2)\n",
    "\n",
    "\n",
    "def surface(Xsim, f1, f2, f3, sigma0):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "\n",
    "    Inside the lambda:\n",
    "    - m a value of moneyness\n",
    "    - tau a time to maturity\n",
    "\n",
    "    Of the function:\n",
    "    - Xsim a simulation of the path under chosen dynamics of the x's\n",
    "    - f1(m, tau) all three are basis functions\n",
    "    - f2(m, tau)\n",
    "    - f3(m, tau)\n",
    "    - sigma0 is a function of the initial volatility for m and tau\n",
    "    Returns:\n",
    "    - Volatility under the three factor model.\n",
    "    \"\"\"\n",
    "    return lambda m, tau: sigma0(m, tau) * np.exp(Xsim[0] * f1(m, tau) + Xsim[1] * f2(m, tau) + Xsim[2] * f3(m, tau))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following pieces of code simulate a path of $(S_t(w_i), \\sigma_t(m,\\tau,w_i);t\\in \\{0, \\dots  t_{max}\\})$ under the following dynamics:\n",
    "\n",
    "$$X_t = (x_t^1, x_t^2, x^3_t)$$\n",
    "a random vector in $R^3$ where the components represent the level, skew and curvature coefficients. \n",
    "Each coordinate is an independent Ornstein-Uhlenbeck procees:\n",
    "$$dx_t^i=-\\lambda^i x_t^i dt +\\gamma^i dW_t^i$$\n",
    "\n",
    "Under an Eurler Scheme:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The price of the underlying is modelled as:\n",
    "$$dS_t = \\sigma_t(1,0)dW_t^0$$\n",
    "where $$W^0 = \\rho W^1 + \\sqrt{1-\\rho^2} B$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simVECTORIALpath(N, lambdaVec, gammaVec, X_0Vect, sigma0, f1, f2, f3, S0, rho = -0.5):\n",
    "    \"\"\"\n",
    "    This function simulates a joint path of the coefficient of the bassi functions together with a path for the underlying price\n",
    "    Inputs:\n",
    "    - N          number of periods to simulate the path in\n",
    "    - lambdaVec  lambda parameters for the OU model\n",
    "    - gammaVec   gamma parameters for the OU model\n",
    "    - X_0Vec     a initial point to simulate the OU under an Euler scheme\n",
    "    - rho        the correlation of the Brownian motions, the default is -0.5 as in the paper\n",
    "    - f1         level basic function\n",
    "    - f2         skew basic function\n",
    "    - f3         curvature basic function\n",
    "    - S0         a initial price of the underlying for the Euler scheme simulation, no default but normally take the price of\n",
    "                the underlying the first day we have available data.\n",
    "\n",
    "    Output:\n",
    "    - Numpy array [S,X] being S the price process simulation and X the X process simulation.\n",
    "    the shape is an array with N entries (periods) of dimension 4 being the first one for each period the pirce S_t.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    h = 1 / 365 #We assume the jump between days is a calendar day distance to avoid over complications\n",
    "    sqrth = np.sqrt(h)\n",
    "\n",
    "    #We first generate the paths for the x^i_t\n",
    "    X = np.zeros((N,3))\n",
    "    # generates a N x 3 matrix of (0, 1) normals\n",
    "    normals =  np.random.normal(size = (N, 3)) \n",
    "    X[0] = X_0Vect #Vector of initial values\n",
    "    \n",
    "    for n in range(1,N):\n",
    "        X[n] = X[n-1] - lambdaVec * X[n-1] * h + gammaVec * normals[n-1] * sqrth\n",
    "\n",
    "\n",
    "    #We now use the path to generate the implied vol surface sigma_t(m, tau)\n",
    "    #In particular we need this to have sigma_t(1,0) to generate the asset path.\n",
    "\n",
    "    sigma_1_0 = np.zeros(N)\n",
    "    sigma_1_0[0] = sigma0\n",
    "\n",
    "    for n in range(1,N):\n",
    "        sigma_1_0[n] = sigma0 * np.exp(X[n,0]*f1(1, 0) + X[n,1]*f2(1, 0) +X[n,2]*f3(1, 0))\n",
    "    \n",
    "    #Now we simulate the Brownian motion that drives the asset:\n",
    "    W = rho * normals[:,0] + np.sqrt(1-rho**2) * np.random.normal(size = N)\n",
    "\n",
    "    #Finally we compute S_t:\n",
    "    S = np.zeros(N)\n",
    "    S[0] = S0\n",
    "    for n in range(1,N):\n",
    "        S[n] = S[n-1] + sigma_1_0[n] * W[n]\n",
    "\n",
    "    result = np.concatenate((S[:, np.newaxis], X), axis=1)\n",
    "    return result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We simulate paths.\n",
    "\n",
    "We require the following parameters:\n",
    "\n",
    "1)Number of paths we want to generate,\n",
    "\n",
    "2)Number of days we want to generate the surfaces for,\n",
    "\n",
    "3)Parameters of the OU dynamics Lambda, takend from paper [9] in the oroginal paper:\n",
    "    R. Cont and J. da Fonseca, Dynamics of implied volatility surfaces, Quant. Finance, 2 (2002), pp. 45â€“60,\n",
    "\n",
    "4)Parameters of the OU dynamics gamma taken from [9],\n",
    "\n",
    "5)Initial point for the simulation of the X random vectors, we need 0 vector so that the       initial point fits the initial volatility used for the simulation,\n",
    "\n",
    "6)Initial \\sigma_0(1,0) needed to generate S_t path, we take f1,\n",
    "\n",
    "7)Initial price of the underlying for the simulation, we will not use it for more than simulating joint dynamics.\n",
    "\n",
    "8)rho is taken straight from the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Usuario\\anaconda3\\lib\\site-packages\\scipy\\interpolate\\_fitpack_impl.py:977: RuntimeWarning: No more knots can be added because the number of B-spline\n",
      "coefficients already exceeds the number of data points m.\n",
      "Probable causes: either s or m too small. (fp>s)\n",
      "\tkx,ky=1,1 nx,ny=20,10 m=136 fp=0.000001 s=0.000000\n",
      "  warnings.warn(RuntimeWarning(_iermess2[ierm][0] + _mess))\n"
     ]
    }
   ],
   "source": [
    "M = 2500                                    #Number of paths we want to generate\n",
    "N = 31                                      #We generate a month of surfaces\n",
    "lambdaVec = np.array([0.035, 0.08, 0.045])  #Parameters of the OU dynamics Lambda\n",
    "gammaVec = 2 * np.sqrt( lambdaVec)          #Parameters of the OU dynamics gamma\n",
    "X_0Vect = [0, 0, 0]                         #Initial point for the simulation of the X random vectors\n",
    "sigma0 = 0.5                                #Initial \\sigma_0(1,0) needed to generate S_t path\n",
    "S0 = 100                                    #Initial price of the underlying for the simulation\n",
    "\n",
    "#Chosen basic functions:\n",
    "def f1(m, tau): #level (it is not dependent of the parameters)\n",
    "    return f1_mean\n",
    "\n",
    "def f2(m, tau): #Skew (it is linear only on moneyness)\n",
    "    return popt2[0] * m + popt2[1]\n",
    "\n",
    "def f3(m, tau): #Curvature (linear in tau, quadratic in moneyness, product of splines)\n",
    "    return (popt3[0] * m**2 + popt3[1] * m + popt3[2]) * (popt3[3] * tau + popt3[4])\n",
    "\n",
    "\n",
    "#Implied volatility in the first period, we can take a empirical observed surface on a particular day and use it as a proxy \n",
    "def initial_surface(df):\n",
    "    x = df['moneyness']\n",
    "    y = df['tau']\n",
    "    z = df['impl_volatility']\n",
    "    f = interpolate.interp2d(x, y, z, kind='linear')\n",
    "    return lambda moneyness, tau: f(moneyness, tau)[0]\n",
    "    \n",
    "sigma_0 = initial_surface(impl_vol_sample)\n",
    "\n",
    "#Create the grid on which we want to construct the volatility\n",
    "gridpoints = 10\n",
    "\n",
    "max_tau = np.max(impl_vol_sample['tau'].unique())\n",
    "min_tau = np.min(impl_vol_sample['tau'].unique())\n",
    "max_moneyness = np.max(impl_vol_sample['moneyness'].unique())\n",
    "min_moneyness = np.min(impl_vol_sample['moneyness'].unique())\n",
    "\n",
    "moneyness = np.linspace(min_moneyness, max_moneyness, gridpoints)\n",
    "taus = np.linspace(min_tau, max_tau, gridpoints)\n",
    "\n",
    "#Create a joint grid\n",
    "mgrid, tgrid = np.meshgrid(moneyness, taus, indexing='ij')\n",
    "\n",
    "\n",
    "paths = []\n",
    "for i in range(M):\n",
    "    path =[]\n",
    "    Xsim = simVECTORIALpath(N, lambdaVec, gammaVec, X_0Vect, sigma0, f1, f2, f3, S0, rho = -0.5)\n",
    "    for n in range(len(Xsim)):\n",
    "        sigma_t = surface([Xsim[n,1], Xsim[n,2], Xsim[n,3]], f1, f2, f3, sigma_0)\n",
    "        triples = []\n",
    "        for m in moneyness:\n",
    "            for tau in taus:\n",
    "                triples.append([m, tau, sigma_t(m, tau)])\n",
    "\n",
    "        triples = np.array(triples)\n",
    "        path.append([Xsim[n,0], triples, n])\n",
    "    paths.append(path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##In the following cells we develop the reweighting approach for Montecarlo simulations, giving lower weights to the penalised paths. The bigger the static arbitrage present, the bigger penalization.\n",
    "\n",
    "A path is a realization of the evolution of the volatility surface:\n",
    "$$(S_t(w_i), \\sigma_t(m,\\tau,w_i);t\\in \\{0, \\dots  t_{max}\\})$$\n",
    "under the measure $\\mathbb{P}_0$ (physical dynamics). \n",
    "\n",
    "Once we have a simulation of $N$ paths we add a weight to each path as following:\n",
    "$$w_i(\\beta):= \\frac{exp(-\\beta \\phi(w_i))}{\\sum_{j=1}^N exp(-\\beta \\phi(w_j))}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "X = np.array(paths, dtype=object)\n",
    "r = 0.07                            #We choose constant interest rate\n",
    "beta = 1.40                         #Penalization coefficient\n",
    "\n",
    "# Create an empty dataframe with desired columns\n",
    "df = pd.DataFrame(columns=['omega', 'time', 'S_t', 'moneyness', 'tau', 'Vol_t'])\n",
    "print(type(df))\n",
    "\n",
    "#We make it a dataframe, it is not the most efficient method, but it is comprenhensible and still works for nt very bug simulations. \n",
    "# Flatten the multi-dimensional array using list comprehension\n",
    "rows = [{'omega': i+1, 'time': X[i][j][2], 'S_t': X[i][j][0], 'moneyness': X[i][j][1][k][0], 'tau': X[i][j][1][k][1], 'Vol_t': X[i][j][1][k][2]} \n",
    "        for i in range(len(X)) for j in range(len(X[i])) for k in range(len(X[i][j][1]))]\n",
    "\n",
    "# Create a data frame from the list of dictionaries\n",
    "df = pd.DataFrame.from_records(rows)\n",
    "\n",
    "# Sort the dataframe by omega, moneyness, and tau\n",
    "df = df.reindex(columns=['omega', 'time', 'S_t', 'tau','moneyness',  'Vol_t']) #for convenience this order is better\n",
    "df = df.sort_values(['omega','time','tau', 'moneyness'])\n",
    "\n",
    "\n",
    "#We calculate this distance: (in this case 0.1)\n",
    "tau_values = df['tau'].unique()\n",
    "jumpt = tau_values[1] - tau_values[0]\n",
    "\n",
    "m_values = df['moneyness'].unique()\n",
    "jumpm = m_values[1] - m_values[0]\n",
    "\n",
    "#Compute the penalties:\n",
    "df['p1'] = df['tau'] * (call(df['moneyness'], df['tau'], df['Vol_t'], r)-call(df['moneyness'], df['tau']+jumpt, df['Vol_t'], r)) /(jumpt)\n",
    "df['p2'] = (call(df['moneyness']+jumpm, df['tau'], df['Vol_t'], r)-call(df['moneyness'], df['tau'], df['Vol_t'], r)) /(jumpm)\n",
    "df['p3'] = (call(df['moneyness'], df['tau'], df['Vol_t'], r)-call(df['moneyness']-jumpm, df['tau'], df['Vol_t'], r)) /(jumpm) - (call(df['moneyness']+jumpm, df['tau'], df['Vol_t'], r)-call(df['moneyness'], df['tau'], df['Vol_t'], r)) /(jumpm)\n",
    "\n",
    "#We only take as penalties the positive values:\n",
    "df['p1'] = df['p1'].apply(lambda x: max(0, x))\n",
    "df['p2'] = df['p2'].apply(lambda x: max(0, x))\n",
    "df['p3'] = df['p3'].apply(lambda x: max(0, x))\n",
    "\n",
    "\n",
    "#We need to make 0 those values that should not have a penalty by \n",
    "#the indexing and have been computing in the previous way for simplicity:\n",
    "\n",
    "# Update the value of p1 to 0 where tau is maximum\n",
    "max_tau = np.max(tau_values)\n",
    "df.loc[df['tau'] == max_tau, 'p1'] = 0\n",
    "\n",
    "# Update the value of p2 to 0 where the indexing needs it to:\n",
    "max_m= np.max(m_values)\n",
    "df.loc[df['moneyness'] == max_m, 'p2'] = 0\n",
    "\n",
    "# Update the value of p3 to 0 where the indexing needs it to:\n",
    "second_index = m_values[0]\n",
    "df.loc[df['moneyness'] == max_m, 'p3'] = 0\n",
    "df.loc[df['moneyness'] == second_index, 'p3'] = 0\n",
    "\n",
    "\n",
    "#After fixing the indexing:\n",
    "df['totalpen'] = df['p1'] + df['p2'] + df['p3']\n",
    "\n",
    "#Group the DataFrame by the 'omega' column and sum the 'p1', 'p2', and 'p3' columns\n",
    "grouped_df = df.groupby('omega')['totalpen'].sum()\n",
    "grouped_df = np.exp(- beta * grouped_df)\n",
    "weights = grouped_df / (grouped_df.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.groupby(['time', 'tau', 'moneyness'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_avg(group):\n",
    "    w = weights[group['omega'].values]\n",
    "    return pd.Series({\n",
    "        'S_t': np.average(group['S_t'], weights=w),\n",
    "        'Vol_t': np.average(group['Vol_t'], weights=w)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg(group):\n",
    "    return pd.Series({\n",
    "        'S_t': np.average(group['S_t']),\n",
    "        'Vol_t': np.average(group['Vol_t'])\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "MC = grouped.apply(avg).reset_index()\n",
    "WMC = grouped.apply(weighted_avg).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = MC\n",
    "\n",
    "#We calculate this distance: (in this case 0.1)\n",
    "tau_values = df['tau'].unique()\n",
    "jumpt = tau_values[1] - tau_values[0]\n",
    "\n",
    "m_values = df['moneyness'].unique()\n",
    "jumpm = m_values[1] - m_values[0]\n",
    "\n",
    "#Compute the penalties:\n",
    "df['p1'] = df['tau'] * (call(df['moneyness'], df['tau'], df['Vol_t'], r)-call(df['moneyness'], df['tau']+jumpt, df['Vol_t'], r)) /(jumpt)\n",
    "df['p2'] = (call(df['moneyness']+jumpm, df['tau'], df['Vol_t'], r)-call(df['moneyness'], df['tau'], df['Vol_t'], r)) /(jumpm)\n",
    "df['p3'] = (call(df['moneyness'], df['tau'], df['Vol_t'], r)-call(df['moneyness']-jumpm, df['tau'], df['Vol_t'], r)) /(jumpm) - (call(df['moneyness']+jumpm, df['tau'], df['Vol_t'], r)-call(df['moneyness'], df['tau'], df['Vol_t'], r)) /(jumpm)\n",
    "\n",
    "#We only take as penalties the positive values:\n",
    "df['p1'] = df['p1'].apply(lambda x: max(0, x))\n",
    "df['p2'] = df['p2'].apply(lambda x: max(0, x))\n",
    "df['p3'] = df['p3'].apply(lambda x: max(0, x))\n",
    "\n",
    "\n",
    "#We need to make 0 those values that should not have a penalty by \n",
    "#the indexing and have been computing in the previous way for simplicity:\n",
    "\n",
    "# Update the value of p1 to 0 where tau is maximum\n",
    "max_tau = np.max(tau_values)\n",
    "df.loc[df['tau'] == max_tau, 'p1'] = 0\n",
    "\n",
    "# Update the value of p2 to 0 where the indexing needs it to:\n",
    "max_m= np.max(m_values)\n",
    "df.loc[df['moneyness'] == max_m, 'p2'] = 0\n",
    "\n",
    "# Update the value of p3 to 0 where the indexing needs it to:\n",
    "second_index = m_values[0]\n",
    "df.loc[df['moneyness'] == max_m, 'p3'] = 0\n",
    "df.loc[df['moneyness'] == second_index, 'p3'] = 0\n",
    "\n",
    "\n",
    "#After fixing the indexing:\n",
    "df['totalpen'] = df['p1'] + df['p2'] + df['p3']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "481.97298240674314"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['totalpen'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = WMC\n",
    "\n",
    "#We calculate this distance: (in this case 0.1)\n",
    "tau_values = df['tau'].unique()\n",
    "jumpt = tau_values[1] - tau_values[0]\n",
    "\n",
    "m_values = df['moneyness'].unique()\n",
    "jumpm = m_values[1] - m_values[0]\n",
    "\n",
    "#Compute the penalties:\n",
    "df['p1'] = df['tau'] * (call(df['moneyness'], df['tau'], df['Vol_t'], r)-call(df['moneyness'], df['tau']+jumpt, df['Vol_t'], r)) /(jumpt)\n",
    "df['p2'] = (call(df['moneyness']+jumpm, df['tau'], df['Vol_t'], r)-call(df['moneyness'], df['tau'], df['Vol_t'], r)) /(jumpm)\n",
    "df['p3'] = (call(df['moneyness'], df['tau'], df['Vol_t'], r)-call(df['moneyness']-jumpm, df['tau'], df['Vol_t'], r)) /(jumpm) - (call(df['moneyness']+jumpm, df['tau'], df['Vol_t'], r)-call(df['moneyness'], df['tau'], df['Vol_t'], r)) /(jumpm)\n",
    "\n",
    "#We only take as penalties the positive values:\n",
    "df['p1'] = df['p1'].apply(lambda x: max(0, x))\n",
    "df['p2'] = df['p2'].apply(lambda x: max(0, x))\n",
    "df['p3'] = df['p3'].apply(lambda x: max(0, x))\n",
    "\n",
    "\n",
    "#We need to make 0 those values that should not have a penalty by \n",
    "#the indexing and have been computing in the previous way for simplicity:\n",
    "\n",
    "# Update the value of p1 to 0 where tau is maximum\n",
    "max_tau = np.max(tau_values)\n",
    "df.loc[df['tau'] == max_tau, 'p1'] = 0\n",
    "\n",
    "# Update the value of p2 to 0 where the indexing needs it to:\n",
    "max_m= np.max(m_values)\n",
    "df.loc[df['moneyness'] == max_m, 'p2'] = 0\n",
    "\n",
    "# Update the value of p3 to 0 where the indexing needs it to:\n",
    "second_index = m_values[0]\n",
    "df.loc[df['moneyness'] == max_m, 'p3'] = 0\n",
    "df.loc[df['moneyness'] == second_index, 'p3'] = 0\n",
    "\n",
    "\n",
    "#After fixing the indexing:\n",
    "df['totalpen'] = df['p1'] + df['p2'] + df['p3']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "465.2100737558249"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['totalpen'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "omega\n",
       "4241    4.087990e-17\n",
       "718     4.557835e-17\n",
       "586     8.236785e-17\n",
       "1357    2.254202e-16\n",
       "588     2.300231e-16\n",
       "            ...     \n",
       "4325    5.803769e-03\n",
       "2561    9.423115e-03\n",
       "1989    1.357363e-02\n",
       "1948    1.398660e-02\n",
       "2974    9.504908e-01\n",
       "Name: totalpen, Length: 5000, dtype: float64"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = np.exp(- beta * df.groupby('omega')['totalpen'].sum()).sum()\n",
    "(np.exp(- beta * df.groupby('omega')['totalpen'].sum())/total).sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000000000000000"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta = 1.59\n",
    "\n",
    "#Group the DataFrame by the 'omega' column and sum the 'p1', 'p2', and 'p3' columns\n",
    "grouped_df = df.groupby('omega')['totalpen'].sum()\n",
    "grouped_df = np.exp(- beta * grouped_df)\n",
    "total = grouped_df.sum()\n",
    "weights = grouped_df / (total)\n",
    "weights.max()-weights.min()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
